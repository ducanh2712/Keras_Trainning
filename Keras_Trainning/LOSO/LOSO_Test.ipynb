{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S1/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S10/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S11/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S12/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S2/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S3/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S4/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S5/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S6/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S7/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S8/train_aug', 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO/S9/train_aug']\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (224, 224)\n",
    "\n",
    "\n",
    "DATA_DIR = 'D:/OneDrive - Hanoi University of Science and Technology/BackUp/EDABK/AI/Data_AI_EDABK/TuTheNgu/EDABK_IR_23CLASS_LOSO'\n",
    "TRAINING_DATA_DIR = []\n",
    "VALID_DATA_DIR = []\n",
    "\n",
    "for fold in os.listdir(DATA_DIR):\n",
    "    TRAINING_DATA_DIR.append(DATA_DIR + '/' + fold + '/' + 'train_aug')\n",
    "    VALID_DATA_DIR.append(DATA_DIR + '/' + fold + '/' + 'test')\n",
    "print(TRAINING_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20052 images belonging to 23 classes.\n",
      "Found 1090 images belonging to 23 classes.\n",
      "Found 20508 images belonging to 23 classes.\n",
      "Found 862 images belonging to 23 classes.\n",
      "Found 20780 images belonging to 23 classes.\n",
      "Found 726 images belonging to 23 classes.\n",
      "Found 20514 images belonging to 23 classes.\n",
      "Found 859 images belonging to 23 classes.\n",
      "Found 20540 images belonging to 23 classes.\n",
      "Found 846 images belonging to 23 classes.\n",
      "Found 20232 images belonging to 23 classes.\n",
      "Found 1000 images belonging to 23 classes.\n",
      "Found 20594 images belonging to 23 classes.\n",
      "Found 819 images belonging to 23 classes.\n",
      "Found 20586 images belonging to 23 classes.\n",
      "Found 823 images belonging to 23 classes.\n",
      "Found 19856 images belonging to 23 classes.\n",
      "Found 1188 images belonging to 23 classes.\n",
      "Found 20402 images belonging to 23 classes.\n",
      "Found 915 images belonging to 23 classes.\n",
      "Found 19936 images belonging to 23 classes.\n",
      "Found 1148 images belonging to 23 classes.\n",
      "Found 20552 images belonging to 23 classes.\n",
      "Found 840 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = []\n",
    "valid_generator = []\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "for i in range(len(os.listdir(DATA_DIR))):\n",
    "    train_generator.append(datagen.flow_from_directory(\n",
    "        TRAINING_DATA_DIR[i],\n",
    "        shuffle=True,\n",
    "        target_size=IMAGE_SHAPE,\n",
    "    ))\n",
    "    valid_generator.append(datagen.flow_from_directory(\n",
    "        VALID_DATA_DIR[i],\n",
    "        shuffle=False,\n",
    "        target_size=IMAGE_SHAPE,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), activation='relu', \n",
    "                           input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "model = build_model(num_classes=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 8)       224       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 8)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 16)      1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 21632)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1384512   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 23)                1495      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,392,039\n",
      "Trainable params: 1,392,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phamd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phamd\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1000\n",
    "history = []\n",
    "\n",
    "\n",
    "for i in range(len(os.listdir(DATA_DIR))):\n",
    "    history.append(model.fit(train_generator[i],\n",
    "                        steps_per_epoch=train_generator[i].samples // BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=valid_generator[i],\n",
    "                        validation_steps= valid_generator[i].samples // BATCH_SIZE,\n",
    "                        verbose=1\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and loss of Fold 0: [0.05781250074505806] [3.128007411956787]\n",
      "Accuracy and loss of Fold 1: [0.06406249850988388] [3.1117427349090576]\n",
      "Accuracy and loss of Fold 2: [0.078125] [3.1057889461517334]\n",
      "Accuracy and loss of Fold 3: [0.07377049326896667] [3.1190242767333984]\n",
      "Accuracy and loss of Fold 4: [0.09531249850988388] [3.102281332015991]\n",
      "Accuracy and loss of Fold 5: [0.0703125] [3.0981931686401367]\n",
      "Accuracy and loss of Fold 6: [0.09218750149011612] [3.0824928283691406]\n",
      "Accuracy and loss of Fold 7: [0.09223300963640213] [3.0748939514160156]\n",
      "Accuracy and loss of Fold 8: [0.09868421405553818] [3.069075107574463]\n",
      "Accuracy and loss of Fold 9: [0.08906249701976776] [3.070026397705078]\n",
      "Accuracy and loss of Fold 10: [0.12171052396297455] [3.0639259815216064]\n",
      "Accuracy and loss of Fold 11: [0.09843750298023224] [3.037937641143799]\n",
      "* Overview:\n",
      "> Accuracy: 0.08597589501490195 (Độ lệch +- 0.017072188929214267)\n",
      "> Loss: 3.088615814844767\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "loss_list = []\n",
    "for i in range(len(os.listdir(DATA_DIR))):\n",
    "    accuracy_list.append(history[i].history['accuracy'] * 100)\n",
    "    loss_list.append(history[i].history['loss'])\n",
    "    print(f'Accuracy and loss of Fold {i}:', (history[i].history['accuracy']), history[i].history['loss'] )\n",
    "    \n",
    "print('* Overview:')\n",
    "print(f'> Accuracy: {np.mean(accuracy_list)} (Độ lệch +- {np.std(accuracy_list)})')\n",
    "print(f'> Loss: {np.mean(loss_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n",
      "Model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training is done!\")\n",
    "model.save('./model/modelLOSO_Test.h5')\n",
    "print(\"Model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - Hanoi University of Science and Technology\\BackUp\\EDABK\\AI\\Keras_Trainning\\LOSO\\LOSO_Test.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Hanoi%20University%20of%20Science%20and%20Technology/BackUp/EDABK/AI/Keras_Trainning/LOSO/LOSO_Test.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(DATA_DIR))):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Hanoi%20University%20of%20Science%20and%20Technology/BackUp/EDABK/AI/Keras_Trainning/LOSO/LOSO_Test.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m history[i]\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20Hanoi%20University%20of%20Science%20and%20Technology/BackUp/EDABK/AI/Keras_Trainning/LOSO/LOSO_Test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_acc \u001b[39m=\u001b[39m history[i]\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(DATA_DIR))):\n",
    "    train_loss = history[i].history['loss']\n",
    "    train_acc = history[i].history['accuracy']\n",
    "    # valid_loss = history[i].history['val_loss']\n",
    "    # valid_acc = history[i].history['val_accuracy']\n",
    "    epoch_list = [*range(EPOCHS)]\n",
    "    print(history[i].history.keys())\n",
    "\n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(epoch_list, history[i].history['accuracy'])\n",
    "    # plt.plot(epoch_list, history[i].history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bab23f09fb65a505bfbe81ca14cc25f1573a18df8f89bc62fc496e048d8d28a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
